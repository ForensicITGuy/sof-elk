# SOF-ELK® Configuration File
# Created by Christophe Vandeplas <christophe@vandeplas.com>
# Please check https://github.com/cvandeplas/ELK-forensics for more information.
# Changes made by Mark Hallman 2019-03-20
# (C)2024 changes made by Lewes Technology Consulting, LLC for the SOF-ELK® platform

# This file contains transforms and enrichments to be applied in postprocessing

filter {
  if [labels][type] == "plaso" {
    csv {
       columns => [ "date", "time", "timezone", "macb", "datasource", "datasourcetype", "eventtype", "username", "host", "short", "desc", "version", "filename", "inode", "notes", "format", "extra" ]
       quote_char => "\x00"
       target => "csv"
    }
    if [csv][date] == "date" {
       drop {}  # drop the first line that contains the column names
    }

    # assemble a string containing "<%date%> <%time%> <%timezone%>" go be used with the date{} filter
    mutate {
      replace => {
        "[csv][date]" => "%{[csv][date]} %{[csv][time]} %{[csv][timezone]}"
      }
    }

    date {
      match => [ "[csv][date]", "MM/dd/YYYY HH:mm:ss z", "MM/dd/YYYY HH:mm:ss ZZZ" ]
    }

    # remove empty fields to prevent odd behavior downstream
    if [csv][username] == "-" {
      mutate {
        remove_field => [ "[csv][username]" ]
      }
    }
    if [csv][inode] == "-" {
      mutate {
        remove_field => [ "[csv][inode]" ]
      }
    }
    if [csv][notes] == "-" {
      mutate {
        remove_field => [ "[csv][notes]" ]
      }
    }

    # PJH TODO: Do we need to unify this with stdinfo.* fields?
    # PJH TODO: Holy crap... should we just alter KAPE to use one record per time value?  how does that affect $FILENAME?
    #           this is an interesting option - it would require using the clone{} filter

    # extract macb info
    if ("M" in [csv][macb]) { mutate { add_tag => [ "modified" ] } }
    if ("A" in [csv][macb]) { mutate { add_tag => [ "accessed" ] } }
    if ("C" in [csv][macb]) { mutate { add_tag => [ "changed" ] } }
    if ("B" in [csv][macb]) { mutate { add_tag => [ "birth" ] } }

    # extract data from the "desc" field based on the respective datasource value
    # Extract filenames
    if [csv][datasource] == "FILE" or [csv][datasource] == "META" {
      grok {
        break_on_match => false
        match => [
          "desc", "(:(?<path>/.*?))?$",
          "path", "(?<filename>[^/]+?)?$"
        ]
      }

    # Extract urls
    # PJH: This does not seem to hit on the sample data used during development
    } else if [csv][datasource] == "WEBHIST" {
      grok {
        match => [
          "desc", "Location: (?<url>.*?)[ $]"
        ]
      }

    # extract event log data fields
    } else if [csv][datasource] == "EVT" and [csv][datasourcetype] == "WinEVTX" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        break_on_match => false
        match => { "[csv][desc]" => [
          "^\[%{POSINT:[winlog][event_id]}%{DATA}?.*\]",
          "Provider identifier: %{GUID:provider_guid}",
          "Source Name: %{DATA:[winlog][provider_name]} ",
          "Strings: \[%{DATA:payload}\]",
          "Computer Name: %{HOSTNAME:[winlog][computer_name]}",
          "Record Number: %{POSINT:record_number}",
          "Event Level: %{POSINT:level}"
          ]
        }
        tag_on_failure => [ "_grokparsefailure_l2t01" ]
      }

      # the xml_string may have important semicolons.  Isolate it for
      # handling, then remove it before we csv against the rest
      if [csv][extra] {
        if [csv][extra] =~ "xml_string: " {
          grok {
            patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
            match => { "[csv][extra]" => [
              "xml_string: %{XMLEVENTSTRING:[csv_extra][xml_string]}"
              ]
            }
          }
          if "[csv_extra][xml_string]" in [tags] {
            mutate {
              gsub => [
                "[csv][extra]", "xml_string: <Event.*/Event>-?", ""
              ]
            }
          }
        }

        kv {
          source => "[csv][extra]"
          target => "kvextra"
          field_split_pattern => "; "
          value_split_pattern => ": "
        }

        mutate {
          rename => {
            "[kvextra][user_sid]" => "[winlog][user][identifier]"
           # "[kvextra][xxx]" => "[xxx]"
          }
        }

        if [csv_extra][xml_string] {
          # For some reason, these have a "-" after each tag-closing ">" which creates a bunch of noise
          mutate {
            gsub => [ "[csv_extra][xml_string]", ">- *", ">" ]
          }

          xml {
            source => "[csv_extra][xml_string]"
            target => "xmldata"
            force_array => false
          }

          # break out eventdata kv pairs to named fields
          # sometimes, though, this data is unnamed
          # this logic looks odd because if there is only one element, it doesn't go into an array.
          # this is due of the force_array setting in the xml load, which is needed for other reasons in this data type
          if [xmldata][EventData][Data] and !([xmldata][EventData][Data][0][Name] or [xmldata][EventData][Data][Name]) {
            mutate {
              convert => [ "[xmldata][EventData][Data]", "string" ]
              rename => {
                "[xmldata][EventData][Data]" => "[xmldata][EventData][string]"
              }
            }

          } else if [xmldata][EventData][Data] {
            ruby {
              path => "/usr/local/sof-elk/supporting-scripts/split_kv_to_fields.rb"
              script_params => {
                "source_field" => "[xmldata][EventData][Data]"
                "destination_field" => "[xmldata][event_data]"
                "key_field" => "Name"
                "val_field" => "content"
              }
            }
          }

          # use this timestamp as a more original source of truth
          if [xmldata][System][TimeCreated][SystemTime] {
            date {
              match => [ "[xmldata][System][TimeCreated][SystemTime]", "ISO8601" ]
            }
          }

          # sometimes, this is in the xml as '<EventID Qualifiers="16384">7040</EventID>', others as '<EventID>7040</EventID>'  handle each case
          if [xmldata][System][EventID][content] {
            mutate {
              rename => {
                "[xmldata][System][EventID][Qualifiers]" => "[winlog][event_id_qualifiers]"
              }
            }
          }
        }

        mutate {
          rename => {
            "[xmldata][System][Execution][ProcessID]" => "[winlog][process][pid]"
            "[xmldata][System][Execution][ThreadID]" => "[winlog][process][thread][id]"
            "[xmldata][System][EventRecordID]" => "[winlog][record_id]"
          }
        }
      }

      # rename anything left and make copies as needed to satisfy ECS needs
      mutate {
        # rename => {
        # }
        copy => {
          "[winlog][computer_name]" => "[host][hostname]"
          "[winlog][provider_name]" => "[event][provider]"
        }
      }

    # extract prefetch data fields
    } else if [csv][datasource] == "LOG" and [csv][datasourcetype] == "WinPrefetch" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        match => [ "[csv][desc]", "Prefetch \[%{DATA:filename}\] was executed - run count %{POSINT:run_count} path: %{DATA:path} hash: %{WORD:prefetch_hash} volume: %{POSINT:volume_number} \[serial number: %{DATA:volume_serial}  device path: %{DATA:device_path}\]" ]
        tag_on_failure => [ "_grokeparsefail_l2t02" ]
      }
    }

    mutate {
      copy => {
        "[winlog][user][identifier]" => "[user][id]"
      }
      rename => {
        "[csv][volume_serial]" => "[volume][serial_number]"
        "[csv][inode]" => "[file][inode]"
      }
    }

    ### TODO: need to replace message with something useful
    ### TODO: need to remove unnecessary fields
    mutate {
      convert => [
        "[level]", "integer",
        "[log][file][device_id]", "integer",
        "[log][file][inode]", "integer",
        "[csv][version]", "integer",
        "[winlog][event_id]", "integer",
        "[winlog][event_id_qualifiers]", "integer",
        "[winlog][process][pid]", "integer",
        "[winlog][process][thread][id]", "integer"
      ]
      remove_field => [ "[xmldata][EventData][Data]", "[xmldata][System][EventID]" ]
      # remove_field => [ "csv", "xmldata", "csv_extra" ]
    }
  }
}
